
# libraries---------------------------------------------------------------------
library(sf)
library(glmmTMB)
library(effects)
library(sjPlot)
library(fitdistrplus)
library(lmtest)

# load and format data----------------------------------------------------------
final <- read.csv("./Data/prePostComplaints.csv", header = TRUE)
final <- final[, -1]

final$ZIP.Code <- as.factor(final$ZIP.Code)
final$Ward <- as.factor(final$Ward)
final$Police.District <- as.factor(final$Police.District)
final$Community.Area <- as.factor(final$Community.Area)

# date formatting
final$Creation.Date <- as.Date(final$Creation.Date, "%Y-%m-%d") 
final$Completion.Date <- as.Date(final$Completion.Date, "%Y-%m-%d") 

# plot true vs false baitings---------------------------------------------------

chicagoBoundary <- st_read("./Data/GIS/chicagoBoundary.shp")

ggplot() + 
  theme_bw() +
  geom_sf(data = chicagoBoundary, color = "black") + 
  geom_point(data = final, 
             aes(x = Longitude, y = Latitude, fill = baited), 
             shape = 21, size = 1, alpha = 0.5)

# regression modeling-----------------------------------------------------------

# https://rpubs.com/kaz_yos/pscl-2
# https://fukamilab.github.io/BIO202/04-C-zero-data.html#zero-inflated_poisson_glm

# NOTE: since it can take a while for rats to die, should we look at complaints 2 weeks post-baiting instead of one week?

# we have count data for our response variable (number of complaints post-baiting)
# usually modeled with Poisson or negative binomial distribution

# poisson
mPois <- glmmTMB(postShort ~ preShort + baited + TAVG + 
                   (1|Community.Area), family = poisson, data = final)
overdisp_fun(mPois)
# indicates overdispersion, suggesting negative binomial might be better

hist(final$postShort)
x <- final$postShort
descdist(x, discrete = TRUE)
fit.nb <- fitdist(x, "nbinom")
plot(fit.nb)
# also suggests a negative binomial distribution

mNB <- glmmTMB(postShort ~ preShort + baited + TAVG + 
                 (1|Community.Area), family = nbinom2, data = final)
overdisp_fun(mNB)
# seems to be a lot better

# we also have a lot of zero values
# are they false or true zeros?

# true zero: no complaints because the baiting worked

# false zero: rats still there, but not observed. or rats there, but not complained about


# "Zero-inflated poisson regression is used to model count data that has an excess of zero counts. Further, theory suggests that the excess zeros are generated by a separate process from the count values and that the excess zeros can be modeled independently."

# "if the overdispersion in a Poisson GLM is caused by the excessive number of zeros, then the ZIP will take care of the overdispersion. But if the overdispersion is not caused by the zeros, then the ZIP is not the appropriate model either"

# suspect that ZINB will be better than ZIP, but can compare with likelihood ratio test

# zero inflated poisson
mZIP <- glmmTMB(postShort ~ preShort + baited + TAVG + (1|Community.Area), 
                zi = ~ preShort + baited + TAVG,
                family = poisson, data = final)

# zero inflated negative binomial
# nbinom2: variance increases quadratically with the mean
mZINB <- glmmTMB(postShort ~ preShort + baited + TAVG + (1|Community.Area), 
                 zi = ~ preShort + baited + TAVG,
                 family = nbinom2, data = final)

lrtest(mZIP, mZINB)

# indicates the ZINB is better

summary(mZINB)
# the zero-inflation model estimates the probability of an extra zero such that a positive contrast indicates a higher chance of absence (e.g. baitedYes < 0 means fewer absences in sites that were baited); this is the opposite of the conditional model where a positive contrast indicates a higher abundance (e.g. baitedYes >0 means higher abundances in sites that were baited)

plot_model(mZINB)
plot(allEffects(mZINB))


# can also fit hurdle models (zero-inflated)
hZIP <- glmmTMB(postShort ~ preShort + baited + TAVG + (1|Community.Area), 
                zi = ~ preShort + baited + TAVG,
                family = truncated_poisson, data = final)

hZINB <- glmmTMB(postShort ~ preShort + baited + TAVG + (1|Community.Area), 
                 zi = ~ preShort + baited + TAVG,
                 family = truncated_nbinom2, data = final)


library(bbmle)
# compare all the GLMMs
AICtab(mPois, mNB, mZIP, mZINB, hZIP, hZINB)

# zero-inflated negative binomial model is best by AIC

# plotting model results--------------------------------------------------------

# https://cran.r-project.org/web/packages/merTools/vignettes/Using_predictInterval.html

## quick and dirty plot
# To avoid marginalizing over or conditioning on random effects, we can refit the best model without the random effect of site; however, this is not ideal because it ignores the correlation within sites.
mZINBfe <- glmmTMB(postShort ~ preShort + baited + TAVG, 
                   zi = ~preShort + baited + TAVG, 
                   family= nbinom2, data = final)
newdata0 <- newdata <- unique(final[, c("preShort", "baited", "TAVG")])
# the predict function has a parameter "type" that specifies whether you want predictions from the conditional model, the zero-inflation model, or the expected response that combines both parts of the model
temp <- predict(mZINBfe, newdata, se.fit = TRUE, type = "response")
newdata$predFE = temp$fit
newdata$predFE.min = temp$fit-1.98*temp$se.fit
newdata$predFE.max = temp$fit+1.98*temp$se.fit

real <- plyr::ddply(final, ~ Community.Area + preShort + baited + TAVG, 
                    summarize, m = mean(postShort))

ggplot(newdata, aes(preShort, predFE)) + geom_point() +
  facet_wrap(~baited) +
  geom_errorbar(aes(ymin = predFE.min, ymax = predFE.max)) +
  geom_point(data=real, colour = "orange", aes(x=preShort, y=m)) +
  ylab("Average abundance \n including presences and absences")+
  xlab("pre Complaints")
# points represent site-specific average counts

## alternative prediction method
# we can predict at the population mode, by setting the random effects to zero

X.cond = model.matrix(lme4::nobars(formula(mZINB)[-2]), newdata0)
beta.cond = fixef(mZINB)$cond
pred.cond = X.cond %*% beta.cond

ziformula = mZINB$modelInfo$allForm$ziformula
X.zi = model.matrix(lme4::nobars(ziformula), newdata0)
beta.zi = fixef(mZINB)$zi
pred.zi = X.zi %*% beta.zi

#These are estimates of the linear predictors (i.e., predictions on the link scale: logit(prob) and log(cond)), not the predictions themselves. The easiest thing to do for the point estimates of the unconditional count (ucount) is to transform to the response scale and multiply:
pred.ucount = exp(pred.cond)*(1-plogis(pred.zi))

# For the standard errors/confidence intervals, we could use posterior predictive simulations (i.e. draw MVN samples from the parameter for the fixed effects). This conditions on/ignores uncertainty in the random-effect parameters.
library(MASS)
set.seed(101)
pred.condpar.psim = mvrnorm(1000, mu=beta.cond, Sigma=vcov(mZINB)$cond)
pred.cond.psim = X.cond %*% t(pred.condpar.psim)
pred.zipar.psim = mvrnorm(1000,mu=beta.zi,Sigma=vcov(mZINB)$zi)
pred.zi.psim = X.zi %*% t(pred.zipar.psim)
pred.ucount.psim = exp(pred.cond.psim)*(1-plogis(pred.zi.psim))
ci.ucount = t(apply(pred.ucount.psim,1,quantile,c(0.025,0.975)))
ci.ucount = data.frame(ci.ucount)
names(ci.ucount) = c("ucount.low","ucount.high")
pred.ucount = data.frame(newdata0, pred.ucount, ci.ucount)

# These predicted counts should be close to the median counts, so we plot them together to compare.
real.count = plyr::ddply(final, ~preShort + baited + TAVG, summarize, m=median(postShort), mu=mean(postShort))

ggplot(pred.ucount, aes(x=preShort, y=pred.ucount))+
  facet_wrap(~baited) +
  geom_point(shape=1, size=2)+
  geom_errorbar(aes(ymin=ucount.low, ymax=ucount.high))+
  geom_point(data=real.count, aes(x=preShort, y=m, colour=baited), 
             shape=0, size=2)+
  # geom_point(data=real.count, aes(x=preShort, y=mu, colour=baited), 
  #            shape=5, size=2)+
  ylab("Abundance \n including presences and absences")+
  xlab("pre Complaints")
# circles represent predicted unconditional counts at the mode (ie community area effect = 0) and error bars represent 95% CIs for that mode. Squares represent the observed median and diamonds represent observed means calculated across samples and community areas



## simulating from a fitted model 

# look at the distribution of simulated values from the best fitted model. for this we use the function simulate.glmmTMB


#https://cran.r-project.org/web/packages/glmmTMB/vignettes/sim.html
sims <- simulate(mZINB, seed = 1)
simdat <- final
simdat$postShort <- sims[[1]]
# simdata <- transform(simdat,
#                      )




sims <- simulate(mZINB, seed = 1, nsim = 10)
# this function returns a list of vectors. the list has one element for each simulation (nsim) and the vectors are the same shape as our response variable
simdatlist=lapply(sims, function(postShort){
  cbind(postShort, final[, c('preShort', 'baited', 'TAVG', 'Community.Area')])
})

# takes forever to run if nsim is high
simdatsums=lapply(simdatlist, function(x){
  plyr::ddply(x, ~preShort+baited+TAVG, summarize,
              absence=mean(postShort==0),
              mu=mean(postShort))
})

ssd=do.call(rbind, simdatsums)

# then we can plot them with the observations summarized in the same way
real = plyr::ddply(final, ~preShort+baited+TAVG, summarize,
                   absence=mean(postShort==0),
                   mu=mean(postShort))

ggplot(ssd, aes(x=absence, color=baited))+
  facet_wrap(~baited) +
  geom_density(adjust=4)+
  geom_point(data=real, aes(x=absence, y=preShort, color=baited), size=2)+
  xlab("Probability that complaints are not made (rats are not observed)") +
  ylab("pre complaints")
# simulated zero counts. densities are values from 10 datasets simulated from best fit model. Points represent the observed data

ggplot(ssd, aes(x=mu, color=baited))+
  geom_density(adjust=4)+
  geom_point(data=real, aes(x=mu, y=.5, color=baited), size=2)+
  xlab("Complaints including zeros")+ylab(NULL)

